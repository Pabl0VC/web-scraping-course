{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.item import Field\n",
    "from scrapy.item import Item\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.loader import ItemLoader\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Articulo(Item):\n",
    "    titulo = Field()\n",
    "    citaciones = Field() \n",
    "    autores = Field()\n",
    "    url = Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleScholar(CrawlSpider):\n",
    "    name = 'googlescholar'\n",
    "    custom_settings = {\n",
    "        'USER_AGENT': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',\n",
    "        'DEPTH_LIMIT': 1, # Para definir que solo se vaya a un nivel de profundidad\n",
    "    }\n",
    "\n",
    "    allowed_domains = ['scholar.google.com']\n",
    "\n",
    "    start_urls = ['https://scholar.google.com/scholar?as_ylo=2023&q=AI&hl=en&as_sdt=0,5']\n",
    "\n",
    "    download_delay = 2\n",
    "\n",
    "    rules = (\n",
    "        Rule( # Regla de movimiento VERTICAL hacia las citaciones de dicho articulo\n",
    "            LinkExtractor(\n",
    "                restrict_xpaths='.//div[@class=\"gs_fl gs_flb\"]',\n",
    "                allow=r'cites=' # \"Si la URL contiene este patron, haz un requerimiento a esa URL\"\n",
    "            ), follow=True, callback=\"parse_start_url\"),\n",
    "    )\n",
    "\n",
    "    # Para extraer informacion desde la url semilla debemos utilizar este nombre de la funion parse_start_url\n",
    "    def parse_start_url(self, response): \n",
    "        sel = Selector(response)\n",
    "\n",
    "        articulos = sel.xpath('.//div[@class=\"gs_ri\"]')\n",
    "        \n",
    "\n",
    "        for articulo in articulos:\n",
    "            item = ItemLoader(Articulo(), articulo)\n",
    "\n",
    "            # Hacemos .getall() debido a que el texto del titulo puede venir dentro de varios tags dentro del <a>\n",
    "            titulo = articulo.xpath('.//h3/a//text()').getall() # .getall() nos devuelve una lista de textos de todos los hijos; que podemos unir \n",
    "            titulo = \"\".join(titulo) \n",
    "\n",
    "            # Obtenemos la URL del articulo\n",
    "            url = articulo.xpath('.//h3/a/@href').get() # extraemos el valor del atributo href. get() solo trae el primer resultado\n",
    "\n",
    "            # Obtenemos los autores del articulo\n",
    "            autores = articulo.xpath('.//div[@class=\"gs_a\"]//text()').getall()\n",
    "            autores = \"\".join(autores)\n",
    "            autores = autores.split('-')[0].strip() # separa la cadena segun guines (-) y se queda solo con el primer elemento y limpia los espacios\n",
    "\n",
    "            # Intentamos obtener el numero de citaciones (ya que no siempre existir√°)\n",
    "            # Por eso inicializamos el valor de la variable en 0 \n",
    "            citaciones = 0\n",
    "            try:\n",
    "                citaciones = articulo.xpath('.//div[@class=\"gs_fl gs_flb\"]/a[contains(@href, \"cites\")]/text()').get() # nos devuelve 'Cited by 23'\n",
    "                citaciones = citaciones.replace('Cited by ', '') # Quitamos el 'Cited by ' para solo quedarnos con el numero\n",
    "            except:\n",
    "                pass #si es que no encuentra el xpath de citaciones no pasa nada y continua el codigo\n",
    "\n",
    "            # En este caso utilizaremos add_value (tambien se puede con add_xpath. La ventaja es que con add_value podemos modificar directamente la extraccion)\n",
    "            item.add_value('titulo', titulo)\n",
    "            item.add_value('citaciones', citaciones)\n",
    "            item.add_value('url', url)\n",
    "            item.add_value('autores', autores)\n",
    "            yield item.load_item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'csv',\n",
    "    'FEED_URI':'data_GoogleScholar.csv',\n",
    "    'FEED_EXPORT_ENCODING':'utf-8',\n",
    "})\n",
    "\n",
    "process.crawl(GoogleScholar)\n",
    "\n",
    "process.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
